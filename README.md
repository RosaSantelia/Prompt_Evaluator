ğŸš€ Prompt Evaluation Tool


This Python script evaluates the quality of generated text by comparing it with reference text using three key metrics:

âœ… BLEU: Measures lexical similarity (word overlap)

âš ï¸ Toxicity: Estimates the probability that the generated text is offensive (using Detoxify)

ğŸ§  Semantic Similarity: Measures how close in meaning two texts are (using Sentence Transformers)


ğŸ› ï¸ Usage

- Install dependencies and all required libraries

- Configure input CSV path

- Set the csv_path variable in the script to point to your CSV file

- The CSV must contain two columns: reference and generated

- Execute the script to compute the evaluation metrics


ğŸ“Š Outputs:

ğŸ¨ Color-coded table displaying BLEU, Toxicity, and Semantic Similarity scores for each prompt.

ğŸ’¾ CSV file named prompt_evaluation_results.csv containing all computed scores.

ğŸ“ˆ Bar chart visualizing the scores across prompts.

![image](https://github.com/user-attachments/assets/ea84ba3f-4edd-4f6d-a316-7a87f80cd5b1)

âš™ï¸ Customization:

âœ‚ï¸ Modify tokenization or smoothing methods for BLEU calculation as needed.

ğŸ”„ Swap toxicity or embedding models for different evaluation needs.

ğŸ“Š Enhance visualizations with additional charts or interactive features.

